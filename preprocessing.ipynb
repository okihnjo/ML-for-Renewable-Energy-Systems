{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read multiple csv from data folder\n",
    "files = []\n",
    "for file in os.listdir('data'):\n",
    "    if file.endswith('.csv'):\n",
    "        files.append(file)\n",
    "print(files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "from shapely.geometry import Point\n",
    "path_to_germany = \"./data/vg2500_geo84/vg2500_bld.shp\"\n",
    "germany_gdf = gpd.read_file(path_to_germany)\n",
    "germany_gdf.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot germany with grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/' + files[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "geometry = [Point(xy) for xy in zip(df.longitude, df.latitude)]\n",
    "geo_df = gpd.GeoDataFrame(df, geometry=geometry)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "germany_gdf.plot(ax=ax, color='lightgrey')\n",
    "\n",
    "geo_df.plot(ax=ax, marker='o', color='red', markersize=5)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Seperate the map of germany into a grid "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate midpoints\n",
    "mid_latitude = df['latitude'].mean()\n",
    "mid_longitude = df['longitude'].mean()\n",
    "\n",
    "def categorize_location(row):\n",
    "    if row['latitude'] >= mid_latitude and row['longitude'] <= mid_longitude:\n",
    "        return 'top_left'\n",
    "    elif row['latitude'] >= mid_latitude and row['longitude'] > mid_longitude:\n",
    "        return 'top_right'\n",
    "    elif row['latitude'] < mid_latitude and row['longitude'] <= mid_longitude:\n",
    "        return 'bottom_left'\n",
    "    else:\n",
    "        return 'bottom_right'\n",
    "\n",
    "# Apply the function to create the new 'location' column\n",
    "df['location'] = df.apply(categorize_location, axis=1)\n",
    "df['location']\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot each location seperately\n",
    "fig, ax = plt.subplots()\n",
    "for i in df['location'].unique():\n",
    "    temp_df = df[df['location'] == i]\n",
    "    ax.scatter(temp_df['longitude'], temp_df['latitude'], label=i)\n",
    "ax.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean dataframe with only important columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(columns=[\"blh\",\"tcc\", \"tsr\", \"sund\", \"tp\", \"fsr\", \"cdir\", \"z\", \"msl\"])\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read measurements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_realized_supply = pd.read_csv('data/' + files[2], sep=';')\n",
    "df_realized_supply.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Again, drop unnecessary columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_realized_supply = df_realized_supply[['Date from', 'Date to', \"Photovoltaic [MW]\", \"Wind Offshore [MW] \", \"Wind Onshore [MW]\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_realized_supply[\"wind_on_offshore\"] = df_realized_supply[\"Wind Offshore [MW] \"] + df_realized_supply[\"Wind Onshore [MW]\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_realized_supply = df_realized_supply.drop(columns=[\"Wind Offshore [MW] \", \"Wind Onshore [MW]\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_realized_supply[\"photo\"] = df_realized_supply[\"Photovoltaic [MW]\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_realized_supply = df_realized_supply.drop(columns=[\"Photovoltaic [MW]\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_realized_supply"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot measurements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "import plotly.express as px"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = go.Figure()\n",
    "x_axis = df_realized_supply[\"Date from\"]\n",
    "\n",
    "fig  = px.line(x=x_axis, y=df_realized_supply.photo,\n",
    "                    )\n",
    "fig.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = go.Figure()\n",
    "x_axis = df_realized_supply[\"Date from\"]\n",
    "\n",
    "fig  = px.line(x=x_axis, y=df_realized_supply.wind_on_offshore,\n",
    "                    )\n",
    "fig.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get year and month from date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_realized_supply[\"timestamps\"] = pd.to_datetime(df_realized_supply[\"Date from\"])\n",
    "df_realized_supply['month_year'] = df_realized_supply['timestamps'].dt.strftime('%Y-%m')\n",
    "df_realized_supply['day'] = df_realized_supply['timestamps'].dt.strftime('%d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_realized_supply"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_ssr(value):\n",
    "    # Remove everything after the comma\n",
    "    value = value.split(',')[0]\n",
    "    # Remove any periods that are used as thousand separators\n",
    "    value = value.replace('.', '')\n",
    "    # Convert to float\n",
    "    return float(value)\n",
    "df_realized_supply[\"photo\"] = df_realized_supply[\"photo\"].apply(preprocess_ssr)\n",
    "df_realized_supply[\"wind_on_offshore\"] = df_realized_supply[\"wind_on_offshore\"].apply(preprocess_ssr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_agg = df_realized_supply.groupby('month_year')[\"photo\"].mean().reset_index()\n",
    "fig = go.Figure()\n",
    "x_axis = df_agg[\"month_year\"]\n",
    "\n",
    "fig  = px.line(x=x_axis, y=df_agg.photo,\n",
    "                    )\n",
    "# title\n",
    "fig.update_layout(\n",
    "    title=\"Average photovoltaic power supply per month\",\n",
    "    xaxis_title=\"Month\",\n",
    "    yaxis_title=\"Power supply [MW]\",\n",
    ")\n",
    "fig.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Take weakly average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_hourly = df_realized_supply.resample('H').photo.mean().reset_index()\n",
    "\n",
    "df_daily = df_hourly.resample('D', on=\"timestamps\").photo.mean().reset_index()\n",
    "\n",
    "df_weekly = df_daily.resample('W', on='timestamps').photo.mean().reset_index()\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_weekly.sort_values(by=\"timestamps\", inplace=True)\n",
    "df_weekly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_agg = df_weekly\n",
    "fig = go.Figure()\n",
    "x_axis = df_agg[\"timestamps\"]\n",
    "\n",
    "fig  = px.line(x=x_axis, y=df_agg.photo,\n",
    "                    )\n",
    "# title\n",
    "fig.update_layout(\n",
    "    title=\"Average photovoltaic power supply per week\",\n",
    "    xaxis_title=\"week\",\n",
    "    yaxis_title=\"Power supply [MW]\",\n",
    ")\n",
    "fig.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_agg = df_hourly\n",
    "fig = go.Figure()\n",
    "x_axis = df_agg[\"timestamps\"]\n",
    "\n",
    "fig  = px.line(x=x_axis, y=df_agg.photo,\n",
    "                    )\n",
    "# title\n",
    "fig.update_layout(\n",
    "    title=\"Average photovoltaic power supply per hour\",\n",
    "    xaxis_title=\"week\",\n",
    "    yaxis_title=\"Power supply [MW]\",\n",
    ")\n",
    "fig.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combine ssr with this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_rows', 10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(columns=[\"longitude\", \"latitude\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop_duplicates()\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"timestamps\"] = pd.to_datetime(df[\"time\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_hourly = df.resample('H', on=\"timestamps\")[\"ssr\"].mean().reset_index()\n",
    "df_daily = df_hourly.resample('D', on=\"timestamps\")[\"ssr\"].mean().reset_index()\n",
    "df_weekly_ssr =df_daily.resample('W', on=\"timestamps\")[\"ssr\"].mean().reset_index()\n",
    "df_weekly_ssr\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## join dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_joined = pd.merge(df_weekly, df_weekly_ssr, on=\"timestamps\", how=\"inner\")\n",
    "df_joined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from darts import TimeSeries\n",
    "from darts.models import KalmanForecaster\n",
    "from darts.datasets import AirPassengersDataset\n",
    "import matplotlib.pyplot as plt\n",
    "from darts.utils.timeseries_generation import datetime_attribute_timeseries\n",
    "\n",
    "series = TimeSeries.from_dataframe(df_joined, 'timestamps', ['photo'])\n",
    "\n",
    "model = KalmanForecaster(dim_x=34)  # Specify the number of components (states)\n",
    "model.fit(series)\n",
    "\n",
    "# Forecast the next 20 time steps\n",
    "forecast = model.predict(60)\n",
    "\n",
    "# Plot the original series and the forecast\n",
    "plt.figure(figsize=(10, 6))\n",
    "series.plot(label='Actual')\n",
    "forecast.plot(label='Forecast')\n",
    "plt.legend()\n",
    "plt.title('Kalman Filter Forecast using N4SID')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_joined.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from darts import TimeSeries\n",
    "from darts.models import KalmanForecaster\n",
    "from darts.datasets import AirPassengersDataset\n",
    "import matplotlib.pyplot as plt\n",
    "from darts.utils.timeseries_generation import datetime_attribute_timeseries\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "series = TimeSeries.from_dataframe(df_joined, 'timestamps', ['photo', \"ssr\"])\n",
    "\n",
    "model = KalmanForecaster(dim_x=34)  # Specify the number of components (states)\n",
    "model.fit(series)\n",
    "\n",
    "# Forecast the next 20 time steps\n",
    "forecast = model.predict(60)\n",
    "\n",
    "# Plot the original series and the forecast\n",
    "plt.figure(figsize=(10, 6))\n",
    "series.plot(label='Actual')\n",
    "forecast.plot(label='Forecast')\n",
    "plt.legend()\n",
    "plt.title('Kalman Filter Forecast using N4SID')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from darts import TimeSeries\n",
    "from darts.models import KalmanForecaster\n",
    "from darts.datasets import AirPassengersDataset\n",
    "import matplotlib.pyplot as plt\n",
    "from darts.utils.timeseries_generation import datetime_attribute_timeseries\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "df_joined[['photo', 'ssr']] = scaler.fit_transform(df_joined[['photo', 'ssr']])\n",
    "series = TimeSeries.from_dataframe(df_joined, 'timestamps', ['photo', \"ssr\"])\n",
    "\n",
    "model = KalmanForecaster(dim_x=34)  # Specify the number of components (states)\n",
    "model.fit(series)\n",
    "\n",
    "# Forecast the next 20 time steps\n",
    "forecast = model.predict(60)\n",
    "\n",
    "# Plot the original series and the forecast\n",
    "plt.figure(figsize=(10, 6))\n",
    "series.plot(label='Actual')\n",
    "forecast.plot(label='Forecast')\n",
    "plt.legend()\n",
    "plt.title('Kalman Filter Forecast using N4SID')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from darts import TimeSeries\n",
    "from darts.models import KalmanForecaster\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Load your data\n",
    "\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaled_values = scaler.fit_transform(df_joined[['photo', 'ssr']])\n",
    "df_joined[['photo', 'ssr']] = scaled_values\n",
    "\n",
    "# Create a TimeSeries object with both 'photo' and 'ssr' columns\n",
    "series = TimeSeries.from_dataframe(df_joined, 'timestamps', ['photo', 'ssr'])\n",
    "\n",
    "# Initialize the KalmanForecaster with the appropriate dimension\n",
    "model = KalmanForecaster(dim_x=34)  # Adjust dim_x based on the complexity needed\n",
    "model.fit(series)\n",
    "\n",
    "# Forecast the next 60 time steps\n",
    "forecast = model.predict(60)\n",
    "\n",
    "# Inverse transform the forecast data to the original scale\n",
    "forecast_df = pd.DataFrame(forecast.pd_dataframe(), columns=['photo', 'ssr'])\n",
    "forecast_inverse = scaler.inverse_transform(forecast_df)\n",
    "\n",
    "# Reconstruct the TimeSeries object from the inverse transformed data\n",
    "forecast_series = TimeSeries.from_dataframe(\n",
    "    pd.DataFrame(forecast_inverse, index=forecast.time_index, columns=['photo', 'ssr']),\n",
    "    time_col=None\n",
    ")\n",
    "\n",
    "# Plot the original series and the forecast\n",
    "plt.figure(figsize=(10, 6))\n",
    "series.plot(label='Actual')\n",
    "forecast_series.plot(label='Forecast')\n",
    "plt.legend()\n",
    "plt.title('lol')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from darts import TimeSeries\n",
    "from darts.models import KalmanForecaster\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Load your data\n",
    "\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaled_values = scaler.fit_transform(df_joined[['photo']])\n",
    "df_joined[['photo']] = scaled_values\n",
    "\n",
    "# Create a TimeSeries object with both 'photo' and 'ssr' columns\n",
    "series = TimeSeries.from_dataframe(df_joined, 'timestamps', ['photo'])\n",
    "\n",
    "# Initialize the KalmanForecaster with the appropriate dimension\n",
    "model = KalmanForecaster(dim_x=34)  # Adjust dim_x based on the complexity needed\n",
    "model.fit(series)\n",
    "\n",
    "# Forecast the next 60 time steps\n",
    "forecast = model.predict(60)\n",
    "\n",
    "# Inverse transform the forecast data to the original scale\n",
    "forecast_df = pd.DataFrame(forecast.pd_dataframe(), columns=['photo'])\n",
    "forecast_inverse = scaler.inverse_transform(forecast_df)\n",
    "\n",
    "# Reconstruct the TimeSeries object from the inverse transformed data\n",
    "forecast_series = TimeSeries.from_dataframe(\n",
    "    pd.DataFrame(forecast_inverse, index=forecast.time_index, columns=['photo']),\n",
    "    time_col=None\n",
    ")\n",
    "\n",
    "# Plot the original series and the forecast\n",
    "plt.figure(figsize=(10, 6))\n",
    "series.plot(label='Actual')\n",
    "forecast.plot(label='Forecast')\n",
    "plt.legend()\n",
    "plt.title('Forecast with only photovoltaic power supply')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Was wurde in der Masterarbeit gemacht ?\n",
    "- Exponential Smoothing um Model zu erstellen\n",
    "- Trend, seasonality und residual\n",
    "- Dabei wurde in jedem Update step von Kalman auch die Parameter des Models geändert\n",
    "- Parameter wurden mit Maximum Likelihood geschätzt\n",
    "- Das Model als State Transition Model\n",
    "- Die Messungen als Observation Model\n",
    "- Das hat nur gut geklappt, weil das rausfinden des zugrundeliegenden Models durch die seasonalität und pattern möglich war\n",
    "- hat auch autocorrelation genutzt um das window für die Tage zu finden - clever\n",
    "\n",
    "## Problem für mich\n",
    "- Exponential smoothing aufwändig\n",
    "- Updaten von 2 Modellen so gesehen\n",
    "- Auch rechenaufwändig (wie in der Masterarbeit beschrieben)\n",
    "- Masterarbeitaufwand vs Seminararbeit 3 ects\n",
    "- Bedarf kompletter Eigenimplementierung ohne Bibliothek\n",
    "\n",
    "## Lösung\n",
    "- Kalman verstanden\n",
    "- Problemstellung verstanden\n",
    "- Warum die Kombi nicht so gut ist in diesem Fall\n",
    "- wann sie gut wäre (und was man machen müsste damit es hier gut ist)\n",
    "- Nutze dennoch darts und erkläre N4SID\n",
    "- Damit hätten wir:\n",
    "    - State Space models\n",
    "    - Kalman Filter\n",
    "    - Usecases wo und wann er gut ist, was die einzelnen Komponenten sind\n",
    "    - Vorgehen\n",
    "    - Bezug auf unser Projekt, inwiefern das hier anwendbar ist\n",
    "    - Lösung: N4SID und Kalman mittels Darts Implementierung\n",
    "    - Fazit\n",
    "\n",
    "\n",
    "## Fragen\n",
    "- Macht es riesen Unterschied ob SSR und Photo oder nur Photo ?\n",
    "- Multivariate vs Univariat ?\n",
    "- Darts Implementierung etwas schwammig, hidden states nicht einsehbar, genauso wie die Kovarianzen - schlimm ?\n",
    "Immerhin beschreibe ich ja was die jeweils machen und wie sie zusammenhängen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
